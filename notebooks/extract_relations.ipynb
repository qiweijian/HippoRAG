{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qwj/code/HippoRAG\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openie_post_ner_instruction = \"\"\"Given a question and its entities, extract and list the triplets found in the question. Use placeholders (#1, #2, etc.) for any void items where the specific entity or detail is not provided.\"\"\"\n",
    "\n",
    "openie_post_ner_frame = \"\"\"\n",
    "Query:\n",
    "```\n",
    "When was the institute that owned The Collegian founded?\n",
    "```\n",
    "{\"named_entities\":[\"The Collegian\"]}\n",
    "\"\"\"\n",
    "ai_output = \"\"\"\n",
    "{\"triplets\": [[\"The Collegian\", \"owned by\", \"#1\"], [\"#1\", \"founded\", \"#2\"]]}\n",
    "\"\"\"\n",
    "\n",
    "real_input_template = \"\"\"\n",
    "Query:\n",
    "```\n",
    "{passage}\n",
    "```\n",
    "{named_entity_json}\n",
    "\"\"\"\n",
    "\n",
    "def llama3_template(passage, named_entity_json):\n",
    "    real_input = real_input_template.format(passage = passage, named_entity_json=named_entity_json)\n",
    "    return \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\" + openie_post_ner_instruction + \"<|eot_id|>\" + \\\n",
    "    \"<|start_header_id|>user<|end_header_id|>\" + openie_post_ner_frame + \"<|eot_id|>\" + \\\n",
    "    \"<|start_header_id|>assistant<|end_header_id|>\" + ai_output + \"<|eot_id|>\" + \\\n",
    "    \"<|start_header_id|>user<|end_header_id|>\" + real_input + \"<|eot_id|>\" + \\\n",
    "     \"<|start_header_id|>assistant<|end_header_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>question</th>\n",
       "      <th>question_decomposition</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_aliases</th>\n",
       "      <th>answerable</th>\n",
       "      <th>triples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2hop__13548_13529</td>\n",
       "      <td>[{'idx': 0, 'title': 'Lionel Messi', 'paragrap...</td>\n",
       "      <td>When was the person who Messi's goals in Copa ...</td>\n",
       "      <td>[{'id': 13548, 'question': \"To whom was Messi'...</td>\n",
       "      <td>June 1982</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>{\"named_entities\": [\"Messi\", \"Copa del Rey\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3hop1__9285_5188_23307</td>\n",
       "      <td>[{'idx': 0, 'title': 'Member states of NATO', ...</td>\n",
       "      <td>What month did the Tripartite discussions begi...</td>\n",
       "      <td>[{'id': 9285, 'question': 'What was the nobili...</td>\n",
       "      <td>mid-June</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>{\"named_entities\": [\"Britain\", \"France\", \"Wars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2hop__766973_770570</td>\n",
       "      <td>[{'idx': 0, 'title': 'Minsk Region', 'paragrap...</td>\n",
       "      <td>What county is Erik Hort's birthplace a part of?</td>\n",
       "      <td>[{'id': 766973, 'question': 'Erik Hort &gt;&gt; plac...</td>\n",
       "      <td>Rockland County</td>\n",
       "      <td>['Rockland County, New York']</td>\n",
       "      <td>True</td>\n",
       "      <td>{\"named_entities\": [\"Erik Hort\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2hop__170823_120171</td>\n",
       "      <td>[{'idx': 0, 'title': 'Blast Corps', 'paragraph...</td>\n",
       "      <td>What year did the publisher of Labyrinth end?</td>\n",
       "      <td>[{'id': 170823, 'question': 'Labyrinth &gt;&gt; publ...</td>\n",
       "      <td>1986</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>{\"named_entities\": [\"Labyrinth\"]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2hop__511454_120259</td>\n",
       "      <td>[{'idx': 0, 'title': 'Kavangoland', 'paragraph...</td>\n",
       "      <td>When was Lady Godiva's birthplace abolished?</td>\n",
       "      <td>[{'id': 511454, 'question': 'Lady Godiva &gt;&gt; pl...</td>\n",
       "      <td>918</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>{\"named_entities\": [\"Lady Godiva's birthplace\"]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      id  \\\n",
       "0           0       2hop__13548_13529   \n",
       "1           1  3hop1__9285_5188_23307   \n",
       "2           2     2hop__766973_770570   \n",
       "3           3     2hop__170823_120171   \n",
       "4           4     2hop__511454_120259   \n",
       "\n",
       "                                          paragraphs  \\\n",
       "0  [{'idx': 0, 'title': 'Lionel Messi', 'paragrap...   \n",
       "1  [{'idx': 0, 'title': 'Member states of NATO', ...   \n",
       "2  [{'idx': 0, 'title': 'Minsk Region', 'paragrap...   \n",
       "3  [{'idx': 0, 'title': 'Blast Corps', 'paragraph...   \n",
       "4  [{'idx': 0, 'title': 'Kavangoland', 'paragraph...   \n",
       "\n",
       "                                            question  \\\n",
       "0  When was the person who Messi's goals in Copa ...   \n",
       "1  What month did the Tripartite discussions begi...   \n",
       "2   What county is Erik Hort's birthplace a part of?   \n",
       "3      What year did the publisher of Labyrinth end?   \n",
       "4       When was Lady Godiva's birthplace abolished?   \n",
       "\n",
       "                              question_decomposition           answer  \\\n",
       "0  [{'id': 13548, 'question': \"To whom was Messi'...        June 1982   \n",
       "1  [{'id': 9285, 'question': 'What was the nobili...         mid-June   \n",
       "2  [{'id': 766973, 'question': 'Erik Hort >> plac...  Rockland County   \n",
       "3  [{'id': 170823, 'question': 'Labyrinth >> publ...             1986   \n",
       "4  [{'id': 511454, 'question': 'Lady Godiva >> pl...              918   \n",
       "\n",
       "                  answer_aliases  answerable  \\\n",
       "0                             []        True   \n",
       "1                             []        True   \n",
       "2  ['Rockland County, New York']        True   \n",
       "3                             []        True   \n",
       "4                             []        True   \n",
       "\n",
       "                                             triples  \n",
       "0  {\"named_entities\": [\"Messi\", \"Copa del Rey\", \"...  \n",
       "1  {\"named_entities\": [\"Britain\", \"France\", \"Wars...  \n",
       "2                  {\"named_entities\": [\"Erik Hort\"]}  \n",
       "3                  {\"named_entities\": [\"Labyrinth\"]}  \n",
       "4   {\"named_entities\": [\"Lady Godiva's birthplace\"]}  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'musique'\n",
    "queries = pd.read_csv(f\"output/{dataset}_queries.named_entity_output.tsv\", sep='\\t')\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "for i, row in queries.iterrows():\n",
    "    passage = row.get('query', row.get('question'))\n",
    "    named_entity_json = row['triples']\n",
    "    prompts.append(llama3_template(passage, named_entity_json))\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>Given a question and its entities, extract and list the triplets found in the question. Use placeholders (#1, #2, etc.) for any void items where the specific entity or detail is not provided.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Query:\n",
      "```\n",
      "When was the institute that owned The Collegian founded?\n",
      "```\n",
      "{\"named_entities\":[\"The Collegian\"]}\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "{\"triplets\": [[\"The Collegian\", \"owned by\", \"#1\"], [\"#1\", \"founded\", \"#2\"]]}\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Query:\n",
      "```\n",
      "When was the person who Messi's goals in Copa del Rey compared to get signed by Barcelona?\n",
      "```\n",
      "{\"named_entities\": [\"Messi\", \"Copa del Rey\", \"Barcelona\"]}\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 12:31:37,267\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 12:31:38 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/data/qwj/model/llama3_8b_instruct_hf', speculative_config=None, tokenizer='/data/qwj/model/llama3_8b_instruct_hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/data/qwj/model/llama3_8b_instruct_hf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-24 12:31:41 utils.py:660] Found nccl from library /home/qwj/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:41 utils.py:660] Found nccl from library /home/qwj/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:42 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:42 selector.py:32] Using XFormers backend.\n",
      "INFO 07-24 12:31:42 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 07-24 12:31:42 selector.py:32] Using XFormers backend.\n",
      "INFO 07-24 12:31:43 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:43 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 07-24 12:31:44 utils.py:132] reading GPU P2P access cache from /home/qwj/.config/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "WARNING 07-24 12:31:44 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:44 utils.py:132] reading GPU P2P access cache from /home/qwj/.config/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m WARNING 07-24 12:31:44 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 07-24 12:31:46 model_runner.py:175] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:47 model_runner.py:175] Loading model weights took 7.4829 GB\n",
      "INFO 07-24 12:31:49 distributed_gpu_executor.py:45] # GPU blocks: 12349, # CPU blocks: 4096\n",
      "INFO 07-24 12:31:51 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-24 12:31:51 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:51 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:51 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-24 12:31:57 model_runner.py:1017] Graph capturing finished in 6 secs.\n",
      "INFO 07-24 12:31:57 block_manager_v1.py:246] Automatic prefix caching is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=2894003)\u001b[0m INFO 07-24 12:31:57 model_runner.py:1017] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "model = LLM(model=\"/data/qwj/model/llama3_8b_instruct_hf\", tensor_parallel_size=2, enable_prefix_caching=True)\n",
    "sampling_params = SamplingParams(\n",
    "    n=1, stop=['<|eot_id|>'], max_tokens=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10/10 [00:01<00:00,  8.83it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(prompts[:10], sampling_params)\n",
    "output_text = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n{\"triplets\": [[\"Messi\", \"goals in\", \"Copa del Rey\"], [\"#1\", \"compared to\", \"#2\"], [\"#2\", \"get signed by\", \"Barcelona\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"Britain\", \"participated in\", \"Tripartite discussions\"], [\"France\", \"participated in\", \"Tripartite discussions\"], [\"#1\", \"originated from\", \"Warsaw Pact\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"Erik Hort\", \"birthplace\", \"#1\"], [\"#1\", \"part of\", \"#2\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"Labyrinth\", \"published by\", \"#1\"], [\"#1\", \"ended\", \"#2\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"Lady Godiva\\'s birthplace\", \"abolished\", \"#1\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"Israel\", \"located in\", \"#1\"], [\"#1\", \"north of\", \"#2\"], [\"#2\", \"location of\", \"#3\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"#1\", \"body of water\", \"Southeast Library\"], [\"Southeast Library\", \"located in\", \"#2\"], [\"#2\", \"empties into\", \"Gulf of Mexico\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"explorer\", \"reach\", \"city\"], [\"city\", \"headquarters of\", \"#1\"], [\"#1\", \"located\", \"#2\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"performer of III\", \"birthplace\", \"#1\"], [\"#1\", \"start of battle\", \"#2\"]]}',\n",
       " '\\n\\n{\"triplets\": [[\"Till dom ensamma\", \"performer of\", \"#1\"], [\"#1\", \"birth date\", \"#2\"]]}']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relations from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json_dict(text):\n",
    "    pattern = r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\})*)*\\})*)*\\}'\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    if match:\n",
    "        json_string = match.group()\n",
    "        try:\n",
    "            json_dict = json.loads(json_string)\n",
    "            return json_dict\n",
    "        except json.JSONDecodeError:\n",
    "            return text\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'triplets': [['Messi', 'goals in', 'Copa del Rey'], ['#1', 'compared to', '#2'], ['#2', 'get signed by', 'Barcelona']]}\n",
      "1 {'triplets': [['Britain', 'participated in', 'Tripartite discussions'], ['France', 'participated in', 'Tripartite discussions'], ['#1', 'originated from', 'Warsaw Pact']]}\n",
      "2 {'triplets': [['Erik Hort', 'birthplace', '#1'], ['#1', 'part of', '#2']]}\n",
      "3 {'triplets': [['Labyrinth', 'published by', '#1'], ['#1', 'ended', '#2']]}\n",
      "4 {'triplets': [[\"Lady Godiva's birthplace\", 'abolished', '#1']]}\n",
      "5 {'triplets': [['Israel', 'located in', '#1'], ['#1', 'north of', '#2'], ['#2', 'location of', '#3']]}\n",
      "6 {'triplets': [['#1', 'body of water', 'Southeast Library'], ['Southeast Library', 'located in', '#2'], ['#2', 'empties into', 'Gulf of Mexico']]}\n",
      "7 {'triplets': [['explorer', 'reach', 'city'], ['city', 'headquarters of', '#1'], ['#1', 'located', '#2']]}\n",
      "8 {'triplets': [['performer of III', 'birthplace', '#1'], ['#1', 'start of battle', '#2']]}\n",
      "9 {'triplets': [['Till dom ensamma', 'performer of', '#1'], ['#1', 'birth date', '#2']]}\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(output_text):\n",
    "    print(i, extract_json_dict(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [00:36<00:00, 27.63it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(prompts, sampling_params)\n",
    "extracted_triples = [extract_json_dict(o.outputs[0].text) for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'paragraphs', 'question', 'question_decomposition',\n",
       "       'answer', 'answer_aliases', 'answerable', 'triples'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = queries.copy()[['Unnamed: 0', 'id', 'question','triples']]\n",
    "data_to_save['extracted_triples'] = extracted_triples\n",
    "data_to_save.to_csv(f\"output/{dataset}_queries.triples.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>triples</th>\n",
       "      <th>extracted_triples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2hop__13548_13529</td>\n",
       "      <td>When was the person who Messi's goals in Copa ...</td>\n",
       "      <td>{\"named_entities\": [\"Messi\", \"Copa del Rey\", \"...</td>\n",
       "      <td>{'triplets': [['#1', 'compared by', 'Messi'], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3hop1__9285_5188_23307</td>\n",
       "      <td>What month did the Tripartite discussions begi...</td>\n",
       "      <td>{\"named_entities\": [\"Britain\", \"France\", \"Wars...</td>\n",
       "      <td>{'triplets': [['the country where', 'the top-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2hop__766973_770570</td>\n",
       "      <td>What county is Erik Hort's birthplace a part of?</td>\n",
       "      <td>{\"named_entities\": [\"Erik Hort\"]}</td>\n",
       "      <td>{'triplets': [['Erik Hort', 'birthplace', '#1'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2hop__170823_120171</td>\n",
       "      <td>What year did the publisher of Labyrinth end?</td>\n",
       "      <td>{\"named_entities\": [\"Labyrinth\"]}</td>\n",
       "      <td>{'triplets': [['Labyrinth', 'published by', '#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2hop__511454_120259</td>\n",
       "      <td>When was Lady Godiva's birthplace abolished?</td>\n",
       "      <td>{\"named_entities\": [\"Lady Godiva's birthplace\"]}</td>\n",
       "      <td>{'triplets': [['Lady Godiva's birthplace', 'ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4hop2__71753_648517_70784_79935</td>\n",
       "      <td>When was the region immediately north of the r...</td>\n",
       "      <td>{\"named_entities\": [\"Israel\", \"Battle of Qurah...</td>\n",
       "      <td>{'triplets': [['#1', 'is located in', 'region ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4hop1__94201_642284_131926_89261</td>\n",
       "      <td>Where does the body of water by the city where...</td>\n",
       "      <td>{\"named_entities\": [\"Southeast Library\", \"Gulf...</td>\n",
       "      <td>{'triplets': [['body of water by the city wher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4hop1__152562_5274_458768_33633</td>\n",
       "      <td>When did the explorer reach the city where the...</td>\n",
       "      <td>{\"named_entities\": [\"Vilaiyaadu Mankatha\"]}</td>\n",
       "      <td>{'triplets': [['#1', 'reached', '#2'], ['#2', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3hop1__305282_282081_73772</td>\n",
       "      <td>When was the start of the battle of the birthp...</td>\n",
       "      <td>{\"named_entities\": [\"battle of the birthplace\"...</td>\n",
       "      <td>{'triplets': [['performer of III', 'performer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2hop__655505_110949</td>\n",
       "      <td>What is the Till dom ensamma performer's birth...</td>\n",
       "      <td>{\"named_entities\": [\"Till dom ensamma\"]}</td>\n",
       "      <td>{'triplets': [['Till dom ensamma', 'has birth ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                id  \\\n",
       "0           0                 2hop__13548_13529   \n",
       "1           1            3hop1__9285_5188_23307   \n",
       "2           2               2hop__766973_770570   \n",
       "3           3               2hop__170823_120171   \n",
       "4           4               2hop__511454_120259   \n",
       "5           5   4hop2__71753_648517_70784_79935   \n",
       "6           6  4hop1__94201_642284_131926_89261   \n",
       "7           7   4hop1__152562_5274_458768_33633   \n",
       "8           8        3hop1__305282_282081_73772   \n",
       "9           9               2hop__655505_110949   \n",
       "\n",
       "                                            question  \\\n",
       "0  When was the person who Messi's goals in Copa ...   \n",
       "1  What month did the Tripartite discussions begi...   \n",
       "2   What county is Erik Hort's birthplace a part of?   \n",
       "3      What year did the publisher of Labyrinth end?   \n",
       "4       When was Lady Godiva's birthplace abolished?   \n",
       "5  When was the region immediately north of the r...   \n",
       "6  Where does the body of water by the city where...   \n",
       "7  When did the explorer reach the city where the...   \n",
       "8  When was the start of the battle of the birthp...   \n",
       "9  What is the Till dom ensamma performer's birth...   \n",
       "\n",
       "                                             triples  \\\n",
       "0  {\"named_entities\": [\"Messi\", \"Copa del Rey\", \"...   \n",
       "1  {\"named_entities\": [\"Britain\", \"France\", \"Wars...   \n",
       "2                  {\"named_entities\": [\"Erik Hort\"]}   \n",
       "3                  {\"named_entities\": [\"Labyrinth\"]}   \n",
       "4   {\"named_entities\": [\"Lady Godiva's birthplace\"]}   \n",
       "5  {\"named_entities\": [\"Israel\", \"Battle of Qurah...   \n",
       "6  {\"named_entities\": [\"Southeast Library\", \"Gulf...   \n",
       "7        {\"named_entities\": [\"Vilaiyaadu Mankatha\"]}   \n",
       "8  {\"named_entities\": [\"battle of the birthplace\"...   \n",
       "9           {\"named_entities\": [\"Till dom ensamma\"]}   \n",
       "\n",
       "                                   extracted_triples  \n",
       "0  {'triplets': [['#1', 'compared by', 'Messi'], ...  \n",
       "1  {'triplets': [['the country where', 'the top-r...  \n",
       "2  {'triplets': [['Erik Hort', 'birthplace', '#1'...  \n",
       "3  {'triplets': [['Labyrinth', 'published by', '#...  \n",
       "4  {'triplets': [['Lady Godiva's birthplace', 'ab...  \n",
       "5  {'triplets': [['#1', 'is located in', 'region ...  \n",
       "6  {'triplets': [['body of water by the city wher...  \n",
       "7  {'triplets': [['#1', 'reached', '#2'], ['#2', ...  \n",
       "8  {'triplets': [['performer of III', 'performer ...  \n",
       "9  {'triplets': [['Till dom ensamma', 'has birth ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_save.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
