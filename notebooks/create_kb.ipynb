{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qwj/code/HippoRAG\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def is_new_files(file_name):\n",
    "    return not (file_name.startswith(\"openie_\") or file_name.startswith('.') or file_name.endswith(\"named_entity_output.tsv\"))\n",
    "\n",
    "def move_files(subfolder_name):\n",
    "    # 定义新子文件夹的名称\n",
    "    new_subfolder = os.path.join('output', subfolder_name)\n",
    "    \n",
    "    # 如果新子文件夹不存在，创建它\n",
    "    if not os.path.exists(new_subfolder):\n",
    "        os.makedirs(new_subfolder)\n",
    "\n",
    "    # 获取文件夹下的所有文件（不包含子文件夹）\n",
    "    for file_name in os.listdir('output'):\n",
    "        file_path = os.path.join('output', file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            # 检查文件名是否以openie_开头或者以named_entity_output.tsv结尾\n",
    "            if is_new_files(file_name):\n",
    "                # 移动文件到新子文件夹\n",
    "                print(f\"Moved: {file_name}\")\n",
    "                shutil.move(file_path, os.path.join(new_subfolder, file_name))\n",
    "\n",
    "def show_new_files():\n",
    "    for file_name in os.listdir('output'):\n",
    "        file_path = os.path.join('output', file_name)\n",
    "    \n",
    "        if os.path.isfile(file_path):\n",
    "            # 检查文件名是否以openie_开头或者以named_entity_output.tsv结尾\n",
    "            if is_new_files(file_name):\n",
    "                print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "read_json = lambda path: json.load(open(path, 'r'))\n",
    "read_pickle = lambda path: pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import TypedDict, List, Dict, NamedTuple, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musique + GPT + COLBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 11656/11656 [00:01<00:00, 10316.67it/s]\n",
      "Correct Wiki Format: 0 out of 11656\n",
      "100%|████████████████████████████████████| 1006/1006 [00:00<00:00, 71231.51it/s]\n"
     ]
    }
   ],
   "source": [
    "!python src/create_graph.py \\\n",
    "    --dataset musique \\\n",
    "    --model_name colbertv2 \\\n",
    "    --extraction_model gpt-3.5-turbo-1106 \\\n",
    "    --extraction_type ner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_to_kb.tsv\n",
      "rel_kb_to_kb.tsv\n",
      "kb_to_kb.tsv\n"
     ]
    }
   ],
   "source": [
    "show_new_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute similar edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kb internal entity similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jul 01, 07:42:44] #> Creating directory colbert/indexes/nbits_2 \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 2 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \"data\\/lm_vectors\\/colbert\\/corpus.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"nbits_2\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-07\\/01\\/07.42.32\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 2,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "[Jul 01, 07:42:48] #> Loading collection...\n",
      "0M \n",
      "[Jul 01, 07:43:00] [0] \t\t # of sampled PIDs = 53085 \t sampled_pids[:3] = [54607, 1332, 39143]\n",
      "[Jul 01, 07:43:00] [0] \t\t #> Encoding 53085 passages..\n",
      "[Jul 01, 07:43:11] [0] \t\t avg_doclen_est = 6.740058422088623 \t len(local_sample) = 53,085\n",
      "[Jul 01, 07:43:11] [0] \t\t Creating 8,192 partitions.\n",
      "[Jul 01, 07:43:11] [0] \t\t *Estimated* 618,258 embeddings.\n",
      "[Jul 01, 07:43:11] [0] \t\t #> Saving the indexing plan to colbert/indexes/nbits_2/plan.json ..\n",
      "Clustering 339907 points in 128D to 8192 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.03 s\n",
      "  Iteration 19 (0.94 s, search 0.72 s): objective=108744 imbalance=1.401 nsplit=0       \n",
      "[Jul 01, 07:43:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 01, 07:43:14] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.045, 0.041, 0.043, 0.042, 0.041, 0.042, 0.04, 0.04, 0.04, 0.043, 0.043, 0.043, 0.04, 0.044, 0.042, 0.043, 0.039, 0.041, 0.041, 0.041, 0.043, 0.042, 0.042, 0.042, 0.04, 0.04, 0.042, 0.041, 0.041, 0.042, 0.041, 0.042, 0.045, 0.041, 0.042, 0.039, 0.04, 0.041, 0.039, 0.04, 0.042, 0.043, 0.043, 0.043, 0.042, 0.043, 0.04, 0.048, 0.041, 0.041, 0.043, 0.041, 0.041, 0.044, 0.041, 0.045, 0.045, 0.041, 0.044, 0.042, 0.039, 0.041, 0.04, 0.045, 0.042, 0.041, 0.043, 0.045, 0.039, 0.04, 0.045, 0.044, 0.043, 0.041, 0.041, 0.044, 0.041, 0.041, 0.043, 0.042, 0.041, 0.041, 0.045, 0.042, 0.04, 0.041, 0.043, 0.042, 0.04, 0.043, 0.041, 0.044, 0.042, 0.043, 0.044, 0.042, 0.046, 0.041, 0.044, 0.042, 0.041, 0.041, 0.039, 0.042, 0.043, 0.041, 0.042, 0.042, 0.043, 0.04, 0.041, 0.04, 0.046, 0.039, 0.042, 0.04, 0.041, 0.042, 0.043, 0.041, 0.042, 0.041, 0.041, 0.041, 0.04, 0.046, 0.043, 0.04]\n",
      "[Jul 01, 07:43:14] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Jul 01, 07:43:14] #> Got bucket_cutoffs = tensor([-0.0323,  0.0001,  0.0327], device='cuda:0') and bucket_weights = tensor([-0.0588, -0.0146,  0.0150,  0.0591], device='cuda:0')\n",
      "[Jul 01, 07:43:14] avg_residual = 0.041900634765625\n",
      "0it [00:00, ?it/s][Jul 01, 07:43:14] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 01, 07:43:19] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 168,451 embeddings. From #0 onward.\n",
      "1it [00:05,  5.32s/it][Jul 01, 07:43:19] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 01, 07:43:24] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 168,687 embeddings. From #25,000 onward.\n",
      "2it [00:10,  5.06s/it][Jul 01, 07:43:24] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 01, 07:43:30] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 168,891 embeddings. From #50,000 onward.\n",
      "3it [00:15,  5.26s/it][Jul 01, 07:43:30] [0] \t\t #> Encoding 16729 passages..\n",
      "[Jul 01, 07:43:33] [0] \t\t #> Saving chunk 3: \t 16,729 passages and 113,058 embeddings. From #75,000 onward.\n",
      "4it [00:19,  4.78s/it]\n",
      "[Jul 01, 07:43:33] [0] \t\t #> Checking all files were saved...\n",
      "[Jul 01, 07:43:33] [0] \t\t Found all files!\n",
      "[Jul 01, 07:43:33] [0] \t\t #> Building IVF...\n",
      "[Jul 01, 07:43:33] [0] \t\t #> Loading codes...\n",
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 2080.25it/s]\n",
      "[Jul 01, 07:43:33] [0] \t\t Sorting codes...\n",
      "[Jul 01, 07:43:33] [0] \t\t Getting unique codes...\n",
      "[Jul 01, 07:43:33] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 01, 07:43:33] #> Building the emb2pid mapping..\n",
      "[Jul 01, 07:43:33] len(emb2pid) = 619087\n",
      "100%|████████████████████████████████████| 8192/8192 [00:00<00:00, 91227.00it/s]\n",
      "[Jul 01, 07:43:34] #> Saved optimized IVF to colbert/indexes/nbits_2/ivf.pid.pt\n",
      "[Jul 01, 07:43:34] [0] \t\t #> Saving the indexing metadata to colbert/indexes/nbits_2/metadata.json ..\n",
      "#> Joined...\n",
      "[Jul 01, 07:43:45] #> Loading collection...\n",
      "0M \n",
      "[Jul 01, 07:43:56] #> Loading codec...\n",
      "[Jul 01, 07:43:56] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 01, 07:43:56] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 01, 07:43:56] #> Loading IVF...\n",
      "[Jul 01, 07:43:57] #> Loading doclens...\n",
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 1322.92it/s]\n",
      "[Jul 01, 07:43:57] #> Loading codes and residuals...\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 290.15it/s]\n",
      "[Jul 01, 07:43:57] #> Loading the queries from data/lm_vectors/colbert/queries.tsv ...\n",
      "[Jul 01, 07:43:57] #> Got 91729 queries. All QIDs are unique.\n",
      "\n",
      "91729it [05:50, 261.35it/s]\n",
      "Saved nearest neighbors to data/lm_vectors/colbert/nearest_neighbor_kb_to_kb.p\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"2,3\" python src/colbertv2_knn.py --filename output/kb_to_kb.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query to kb similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jul 01, 07:53:18] #> Note: Output directory colbert/indexes/nbits_2 already exists\n",
      "\n",
      "\n",
      "[Jul 01, 07:53:18] #> Will delete 22 files already at colbert/indexes/nbits_2 in 20 seconds...\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 2 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \"data\\/lm_vectors\\/colbert\\/corpus.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"nbits_2\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-07\\/01\\/07.53.06\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 2,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "[Jul 01, 07:53:42] #> Loading collection...\n",
      "0M \n",
      "[Jul 01, 07:53:54] [0] \t\t # of sampled PIDs = 53173 \t sampled_pids[:3] = [54607, 1332, 39143]\n",
      "[Jul 01, 07:53:54] [0] \t\t #> Encoding 53173 passages..\n",
      "[Jul 01, 07:54:05] [0] \t\t avg_doclen_est = 6.753013610839844 \t len(local_sample) = 53,173\n",
      "[Jul 01, 07:54:05] [0] \t\t Creating 8,192 partitions.\n",
      "[Jul 01, 07:54:05] [0] \t\t *Estimated* 621,506 embeddings.\n",
      "[Jul 01, 07:54:05] [0] \t\t #> Saving the indexing plan to colbert/indexes/nbits_2/plan.json ..\n",
      "Clustering 341125 points in 128D to 8192 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.03 s\n",
      "  Iteration 19 (0.96 s, search 0.72 s): objective=109534 imbalance=1.398 nsplit=0       \n",
      "[Jul 01, 07:54:08] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 01, 07:54:09] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.045, 0.042, 0.043, 0.042, 0.042, 0.042, 0.04, 0.04, 0.04, 0.043, 0.043, 0.042, 0.04, 0.044, 0.043, 0.043, 0.04, 0.04, 0.041, 0.041, 0.043, 0.042, 0.041, 0.042, 0.04, 0.041, 0.042, 0.041, 0.041, 0.042, 0.041, 0.042, 0.046, 0.041, 0.042, 0.039, 0.041, 0.041, 0.04, 0.04, 0.042, 0.043, 0.043, 0.043, 0.042, 0.043, 0.039, 0.047, 0.041, 0.041, 0.043, 0.041, 0.04, 0.044, 0.041, 0.045, 0.045, 0.041, 0.044, 0.042, 0.039, 0.04, 0.04, 0.044, 0.042, 0.042, 0.043, 0.045, 0.039, 0.041, 0.045, 0.044, 0.042, 0.04, 0.041, 0.044, 0.041, 0.041, 0.043, 0.042, 0.041, 0.04, 0.044, 0.043, 0.04, 0.04, 0.043, 0.041, 0.04, 0.043, 0.041, 0.044, 0.042, 0.043, 0.044, 0.042, 0.046, 0.042, 0.043, 0.042, 0.04, 0.041, 0.039, 0.042, 0.042, 0.041, 0.042, 0.042, 0.042, 0.04, 0.041, 0.041, 0.046, 0.039, 0.042, 0.04, 0.041, 0.042, 0.043, 0.041, 0.042, 0.041, 0.041, 0.042, 0.04, 0.046, 0.043, 0.041]\n",
      "[Jul 01, 07:54:09] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Jul 01, 07:54:09] #> Got bucket_cutoffs = tensor([-0.0323,  0.0002,  0.0327], device='cuda:0') and bucket_weights = tensor([-0.0587, -0.0146,  0.0150,  0.0592], device='cuda:0')\n",
      "[Jul 01, 07:54:09] avg_residual = 0.041839599609375\n",
      "0it [00:00, ?it/s][Jul 01, 07:54:09] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 01, 07:54:14] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 168,215 embeddings. From #0 onward.\n",
      "1it [00:05,  5.50s/it][Jul 01, 07:54:14] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 01, 07:54:20] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 168,708 embeddings. From #25,000 onward.\n",
      "2it [00:10,  5.31s/it][Jul 01, 07:54:20] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 01, 07:54:24] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 169,034 embeddings. From #50,000 onward.\n",
      "3it [00:15,  5.11s/it][Jul 01, 07:54:24] [0] \t\t #> Encoding 17034 passages..\n",
      "[Jul 01, 07:54:28] [0] \t\t #> Saving chunk 3: \t 17,034 passages and 115,117 embeddings. From #75,000 onward.\n",
      "4it [00:18,  4.74s/it]\n",
      "[Jul 01, 07:54:28] [0] \t\t #> Checking all files were saved...\n",
      "[Jul 01, 07:54:28] [0] \t\t Found all files!\n",
      "[Jul 01, 07:54:28] [0] \t\t #> Building IVF...\n",
      "[Jul 01, 07:54:28] [0] \t\t #> Loading codes...\n",
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 2380.42it/s]\n",
      "[Jul 01, 07:54:28] [0] \t\t Sorting codes...\n",
      "[Jul 01, 07:54:28] [0] \t\t Getting unique codes...\n",
      "[Jul 01, 07:54:28] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 01, 07:54:28] #> Building the emb2pid mapping..\n",
      "[Jul 01, 07:54:28] len(emb2pid) = 621074\n",
      "100%|████████████████████████████████████| 8192/8192 [00:00<00:00, 93068.91it/s]\n",
      "[Jul 01, 07:54:28] #> Saved optimized IVF to colbert/indexes/nbits_2/ivf.pid.pt\n",
      "[Jul 01, 07:54:28] [0] \t\t #> Saving the indexing metadata to colbert/indexes/nbits_2/metadata.json ..\n",
      "#> Joined...\n",
      "[Jul 01, 07:54:39] #> Loading collection...\n",
      "0M \n",
      "[Jul 01, 07:54:51] #> Loading codec...\n",
      "[Jul 01, 07:54:51] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 01, 07:54:51] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 01, 07:54:51] #> Loading IVF...\n",
      "[Jul 01, 07:54:51] #> Loading doclens...\n",
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 1289.46it/s]\n",
      "[Jul 01, 07:54:51] #> Loading codes and residuals...\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 303.60it/s]\n",
      "[Jul 01, 07:54:51] #> Loading the queries from data/lm_vectors/colbert/queries.tsv ...\n",
      "[Jul 01, 07:54:51] #> Got 1234 queries. All QIDs are unique.\n",
      "\n",
      "1234it [00:04, 247.26it/s]\n",
      "Saved nearest neighbors to data/lm_vectors/colbert/nearest_neighbor_query_to_kb.p\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"2,3\" python src/colbertv2_knn.py --filename output/query_to_kb.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.tsv\t\t     nearest_neighbor_query_to_kb.p\n",
      "nearest_neighbor_kb_to_kb.p  queries.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls data/lm_vectors/colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91729"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_to_kb_sim: Dict[str, Tuple[List[str], List[float]]] = read_pickle(\"data/lm_vectors/colbert/nearest_neighbor_kb_to_kb.p\")\n",
    "len(kb_to_kb_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_to_kb_sim: Dict[str, Tuple[List[str], List[float]]] = read_pickle(\"data/lm_vectors/colbert/nearest_neighbor_query_to_kb.p\")\n",
    "len(query_to_kb_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_graph`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 11656/11656 [00:01<00:00, 10829.59it/s]\n",
      "Correct Wiki Format: 0 out of 11656\n",
      "100%|████████████████████████████████████| 1006/1006 [00:00<00:00, 69863.40it/s]\n",
      "Creating Graph\n",
      "100%|██████████████████████████████████| 11656/11656 [00:00<00:00, 11829.07it/s]\n",
      "Loading Vectors\n",
      "Augmenting Graph from Similarity\n",
      "100%|██████████████████████████████████| 91729/91729 [00:01<00:00, 66869.42it/s]\n",
      "Saving Graph\n",
      "                                                         1\n",
      "0                                                         \n",
      "Total Phrases                                       327435\n",
      "Unique Phrases                                       91729\n",
      "Number of Individual Triples                        109145\n",
      "Number of Incorrectly Formatted Triples (ChatGP...    1136\n",
      "Number of Triples w/o NER Entities (ChatGPT Error)    6762\n",
      "Number of Unique Individual Triples                 107448\n",
      "Number of Entities                                  218290\n",
      "Number of Relations                                 294610\n",
      "Number of Unique Entities                            91729\n",
      "Number of Synonymy Edges                            191146\n",
      "Number of Unique Relations                           21714\n"
     ]
    }
   ],
   "source": [
    "!python src/create_graph.py \\\n",
    "    --dataset musique \\\n",
    "    --model_name colbertv2 \\\n",
    "    --extraction_model gpt-3.5-turbo-1106 \\\n",
    "    --extraction_type ner \\\n",
    "    --threshold 0.8 \\\n",
    "    --create_graph \\\n",
    "    --cosine_sim_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musique_facts_and_sim_graph_clean_facts_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "musique_similarity_edges_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n",
      "musique_facts_and_sim_graph_nodes_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "query_to_kb.tsv\n",
      "rel_kb_to_kb.tsv\n",
      "musique_facts_and_sim_graph_fact_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "musique_facts_and_sim_graph_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "musique_facts_and_sim_graph_passage_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "musique_facts_and_sim_graph_fact_doc_edges_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "kb_to_kb.tsv\n",
      "musique_facts_and_sim_graph_doc_to_facts_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "musique_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "musique_facts_and_sim_graph_doc_to_facts_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "musique_facts_and_sim_graph_facts_to_phrases_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "musique_facts_and_sim_graph_facts_to_phrases_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "musique_facts_and_sim_graph_relation_dict_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n",
      "musique_facts_and_sim_graph_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n"
     ]
    }
   ],
   "source": [
    "show_new_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: musique_facts_and_sim_graph_clean_facts_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Moved: musique_similarity_edges_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_nodes_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Moved: query_to_kb.tsv\n",
      "Moved: rel_kb_to_kb.tsv\n",
      "Moved: musique_facts_and_sim_graph_fact_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Moved: musique_facts_and_sim_graph_passage_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Moved: musique_facts_and_sim_graph_fact_doc_edges_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: kb_to_kb.tsv\n",
      "Moved: musique_facts_and_sim_graph_doc_to_facts_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_doc_to_facts_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_facts_to_phrases_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_facts_to_phrases_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_relation_dict_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n",
      "Moved: musique_facts_and_sim_graph_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n"
     ]
    }
   ],
   "source": [
    "move_files('musique_gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./output/musique_gpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109145\n",
      "[{'idx': 0, 'head': 'lionel messi', 'relation': 'enrolled in', 'tail': 'royal spanish football federation'}, {'idx': 1, 'head': 'lionel messi', 'relation': 'befriended', 'tail': 'cesc f bregas'}, {'idx': 2, 'head': 'lionel messi', 'relation': 'befriended', 'tail': 'gerard piqu'}, {'idx': 3, 'head': 'lionel messi', 'relation': 'part of', 'tail': 'baby dream team'}, {'idx': 4, 'head': 'lionel messi', 'relation': 'played for', 'tail': 'barcelona'}]\n"
     ]
    }
   ],
   "source": [
    "class TripletINFO(TypedDict):\n",
    "    idx: int\n",
    "    head: str\n",
    "    relation: str\n",
    "    tail: str\n",
    "\n",
    "triplet_list: List[TripletINFO] = read_json(base_dir+\"/musique_facts_and_sim_graph_clean_facts_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\")\n",
    "print(len(triplet_list))\n",
    "print(triplet_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fact dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107448"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Triplet(NamedTuple):\n",
    "    head: str\n",
    "    relation: str\n",
    "    tail: str\n",
    "\n",
    "raw_triplet_to_idx = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_fact_dict_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "triplet_to_idx = {\n",
    "    Triplet(head=k[0], relation=k[1], tail=k[2]): v for k, v in raw_triplet_to_idx.items()\n",
    "}\n",
    "len(triplet_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_triplet = {v:k for k, v in triplet_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109145, 107448)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_list_2 = [Triplet(head=t['head'], relation=t['relation'], tail=t['tail']) for t in triplet_list]\n",
    "len(triplet_list_2), len(set(triplet_list_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91729\n",
      "[{'idx': 0, 'name': ''}, {'idx': 1, 'name': '0'}, {'idx': 2, 'name': '0    0'}, {'idx': 3, 'name': '0    2'}, {'idx': 4, 'name': '0   1 972'}]\n"
     ]
    }
   ],
   "source": [
    "class EntityINFO(TypedDict):\n",
    "    idx: str\n",
    "    name: str\n",
    "\n",
    "entity_list: List[EntityINFO] = read_json(base_dir+\"/musique_facts_and_sim_graph_nodes_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\")\n",
    "print(len(entity_list))\n",
    "print(entity_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91729\n"
     ]
    }
   ],
   "source": [
    "class SimilarEntities(NamedTuple):\n",
    "    name: str\n",
    "    sim_score: float\n",
    "\n",
    "similar_entities: List[Tuple[str, List[SimilarEntities]]] = read_pickle(f\"{base_dir}/musique_similarity_edges_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\")\n",
    "print(len(similar_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91729"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph: Dict[str, Dict[str, Tuple[str, float]]] = read_json(f\"{base_dir}/musique_facts_and_sim_graph_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\")\n",
    "len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lionel messi {'royal spanish football federation': ['triple', 1], 'cesc f bregas': ['triple', 1], 'gerard piqu': ['triple', 1], 'baby dream team': ['triple', 1], 'barcelona': ['triple', 5], 'cadetes a': ['triple', 1], 'league': ['triple', 1], 'spanish cup': ['triple', 1], 'catalan cup': ['triple', 1], 'arsenal': ['triple', 1], 'fc barcelona': ['triple', 1], 'top scorer': ['triple', 1], '46 goals in the league': ['triple', 1], 'two hat tricks': ['triple', 1], 'barcelona s 5    4 victory over sevilla': ['triple', 1], 'defeat against athletic bilbao': ['triple', 1], 'uefa champions league': ['triple', 1], 'barcelona s match against las palmas': ['triple', 1], 'barcelona s 4    0 away win over real madrid in el cl sico': ['triple', 1], '2015 fifa club world cup final': ['triple', 1], 'silver ball': ['triple', 1], 'barcelona in a 4    0 home win over real betis': ['triple', 1], 'rosario  argentina': ['triple', 1], 'fifa u 20 world cup': ['triple', 1], 'copa am rica': ['triple', 2], 'olympic games': ['triple', 2], 'fifa world cup': ['triple', 1], 'argentina': ['triple', 1], 'newell s old boys': ['triple', 1], 'el cl sico': ['triple', 1], 'all time top goalscorer of la liga': ['triple', 1], '50 goals in 2011 12 season': ['triple', 1], 'el cl sico  26': ['triple', 1], 'association football forward': ['triple', 1], 'argentina national football team': ['triple', 1], 'argentina in 2005': ['triple', 1], '65 goals': ['triple', 1], '17 august 2005': ['triple', 1], 'gabriel batistuta s record': ['triple', 1], 'gabriel batistuta': ['triple', 1], 'most goals scored in a season': ['triple', 1], '50 goals in 2011 12': ['triple', 1], '21 goals in fifa world cup qualifiers': ['triple', 1], 'eight goals in copa am rica': ['triple', 1], 'best player award': ['triple', 1], 'six times in the fifa world cup tournaments': ['triple', 1], 'his team to the final in 2014': ['triple', 1], 'golden ball in 2014': ['triple', 1], 'once in the 2018 world cup finals': ['triple', 1], '30 goals in friendlies': ['triple', 1], 'luis su rez': ['triple', 1], 'conmebol': ['triple', 1], 'copa am rica in 2007': ['triple', 1], 'copa am rica in 2015': ['triple', 1], 'copa am rica in 2016': ['triple', 1]}\n"
     ]
    }
   ],
   "source": [
    "for key, value in itertools.islice(graph.items(),3):\n",
    "    print(key, value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11656\n"
     ]
    }
   ],
   "source": [
    "# musique_facts_graph_passage_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
    "class PassageINFO(TypedDict):\n",
    "    title: str\n",
    "    text: str\n",
    "    passage: str\n",
    "    extracted_entities: List[str]\n",
    "    extracted_triples: List[tuple]\n",
    "    entities: List[tuple]\n",
    "    clean_triples: List[tuple]\n",
    "    noisy_triples: List[tuple]\n",
    "\n",
    "graph_passage: List[PassageINFO] = read_json(f\"{base_dir}/musique_facts_and_sim_graph_passage_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\")\n",
    "print(len(graph_passage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91729"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# musique_facts_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
    "entity_to_id: Dict[str, int] = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "len(entity_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206863"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_edges: Dict[Tuple[int, int], float] = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_fact_doc_edges_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "len(doc_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc_to_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108914"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dict\n",
    "doc_to_facts: Dict[Tuple[int, int], float] = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_doc_to_facts_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "len(doc_to_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(doc_to_facts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11656, 109145)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_fact_matrix: csr_array = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_doc_to_facts_csr_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "\n",
    "doc_to_fact_matrix.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### facts to phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214783"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dict\n",
    "facts_to_phrases: Dict[Tuple[int, int], float] = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_facts_to_phrases_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "len(facts_to_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109145, 91729)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts_to_phrases_matrix: csr_array = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_facts_to_phrases_csr_ents_only_lower_preprocess_ner.v3.subset.p\")\n",
    "\n",
    "facts_to_phrases_matrix.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394781"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_graph: Dict[Tuple[int, int], float] = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\")\n",
    "len(entity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 11.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(entity_graph.values()), max(entity_graph.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91728, 91728)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = [k[0] for k in entity_graph.keys()]\n",
    "end_idx = [k[1] for k in entity_graph.keys()]\n",
    "\n",
    "max(start_idx), max(end_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294610"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict: Dict[Tuple[str, str], str] = read_pickle(f\"{base_dir}/musique_facts_and_sim_graph_relation_dict_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\")\n",
    "len(relation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lionel messi', 'royal spanish football federation'),\n",
       " ('lionel messi', 'cesc f bregas'),\n",
       " ('lionel messi', 'gerard piqu'),\n",
       " ('lionel messi', 'baby dream team'),\n",
       " ('lionel messi', 'barcelona'),\n",
       " ('lionel messi', 'cadetes a'),\n",
       " ('lionel messi', 'league'),\n",
       " ('lionel messi', 'spanish cup'),\n",
       " ('lionel messi', 'catalan cup'),\n",
       " ('lionel messi', 'arsenal')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(relation_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187918"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict_cp  = relation_dict.copy()\n",
    "for t in triplet_list:\n",
    "    edge = (t['head'], t['tail'])\n",
    "    if edge in relation_dict_cp:\n",
    "        relation_dict_cp.pop(edge)\n",
    "    edge = (t['tail'], t['head'])\n",
    "    if edge in relation_dict_cp:\n",
    "        relation_dict_cp.pop(edge)\n",
    "len(relation_dict_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'equivalent'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(relation_dict_cp.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New KG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building knowledge graph: 100%|██████████| 298594/298594 [00:01<00:00, 149850.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeBase(91729 entities, 22222 relations, 298594 triplets)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "from kb_utils import ExperimentConfig, KnowledgeBase\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    dataset='musique',\n",
    "    graph_type='facts_and_sim',\n",
    "    retrieval_model_name='colbertv2',\n",
    "    extraction_model_name='gpt-3.5-turbo-1106',\n",
    "    base_dir='./output/musique_gpt'\n",
    ")\n",
    "kb = KnowledgeBase.build_from_config(config)\n",
    "print(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entity(name=pam, 31 relations)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.kb[64165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TargetEntity(-- breastfeeding -> cecelia),\n",
       " TargetEntity(-- equivalent -> jim and pam),\n",
       " TargetEntity(-- equivalent -> pam beesly),\n",
       " TargetEntity(-- equivalent -> pam belluck),\n",
       " TargetEntity(-- equivalent -> pam faris),\n",
       " TargetEntity(-- equivalent -> pam for dinner),\n",
       " TargetEntity(-- equivalent -> pam grier),\n",
       " TargetEntity(-- equivalent -> pam martin),\n",
       " TargetEntity(-- equivalent -> pam s drawing),\n",
       " TargetEntity(-- equivalent -> pam s graphic design projects),\n",
       " TargetEntity(-- equivalent -> pam s mother),\n",
       " TargetEntity(-- equivalent -> pam s voicemail messages),\n",
       " TargetEntity(-- equivalent -> pam s voicemails),\n",
       " TargetEntity(-- equivalent -> pam s work),\n",
       " TargetEntity(-- equivalent -> pam shriver),\n",
       " TargetEntity(-- equivalent -> pam slapping michael),\n",
       " TargetEntity(-- equivalent -> pam tillis),\n",
       " TargetEntity(-- equivalent -> pamal broadcasting),\n",
       " TargetEntity(-- equivalent -> pambula river),\n",
       " TargetEntity(-- is -> let know by jim that he is seeing someone),\n",
       " TargetEntity(-- is -> overjoyed),\n",
       " TargetEntity(-- is married to -> jim),\n",
       " TargetEntity(-- left -> voicemail messages for jim),\n",
       " TargetEntity(-- nurses -> baby),\n",
       " TargetEntity(-- nurses -> cecelia),\n",
       " TargetEntity(<- accidentally calls -- jim),\n",
       " TargetEntity(<- gets the car for -- jim),\n",
       " TargetEntity(<- is husband of -- jim),\n",
       " TargetEntity(<- is supportive of -- jim),\n",
       " TargetEntity(<- marry -- jim),\n",
       " TargetEntity(<- proposes to -- jim)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.kb[64165].relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
