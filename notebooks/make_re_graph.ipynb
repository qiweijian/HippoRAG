{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qwj/code/HippoRAG\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def is_new_files(file_name):\n",
    "    return not (file_name.startswith(\"openie_\") or file_name.startswith('.') or file_name.endswith(\"named_entity_output.tsv\") or file_name.endswith(\"entity_and_relations.tsv\"))\n",
    "\n",
    "def move_files(subfolder_name):\n",
    "    # 定义新子文件夹的名称\n",
    "    new_subfolder = os.path.join('output', subfolder_name)\n",
    "    \n",
    "    # 如果新子文件夹不存在，创建它\n",
    "    if not os.path.exists(new_subfolder):\n",
    "        os.makedirs(new_subfolder)\n",
    "\n",
    "    # 获取文件夹下的所有文件（不包含子文件夹）\n",
    "    for file_name in os.listdir('output'):\n",
    "        file_path = os.path.join('output', file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            # 检查文件名是否以openie_开头或者以named_entity_output.tsv结尾\n",
    "            if is_new_files(file_name):\n",
    "                # 移动文件到新子文件夹\n",
    "                print(f\"Moved: {file_name}\")\n",
    "                shutil.move(file_path, os.path.join(new_subfolder, file_name))\n",
    "\n",
    "def show_new_files():\n",
    "    for file_name in os.listdir('output'):\n",
    "        file_path = os.path.join('output', file_name)\n",
    "    \n",
    "        if os.path.isfile(file_path):\n",
    "            # 检查文件名是否以openie_开头或者以named_entity_output.tsv结尾\n",
    "            if is_new_files(file_name):\n",
    "                print(file_name)\n",
    "\n",
    "def remove_new_files():\n",
    "    for file_name in os.listdir('output'):\n",
    "        file_path = os.path.join('output', file_name)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            # 检查文件名是否以openie_开头或者以named_entity_output.tsv结尾\n",
    "            if is_new_files(file_name):\n",
    "                print(f\"Removed: {file_name}\")\n",
    "                os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Create Retriever Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 11656/11656 [00:01<00:00, 10142.54it/s]\n",
      "Correct Wiki Format: 0 out of 11656\n"
     ]
    }
   ],
   "source": [
    "!python src/create_graph_v2.py \\\n",
    "    --dataset musique \\\n",
    "    --model_name colbertv2 \\\n",
    "    --extraction_model gpt-3.5-turbo-1106 \\\n",
    "    --threshold 0.8 \\\n",
    "    --extraction_type ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_to_kb.tsv\n",
      "rel_kb_to_kb.tsv\n",
      "musique_queries.entity_and_relations.tsv\n",
      "kb_to_kb.tsv\n"
     ]
    }
   ],
   "source": [
    "show_new_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute similar edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kb internal similarity\n",
    "\n",
    "here we have entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jul 05, 04:03:29] #> Creating directory colbert/indexes/nbits_2 \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \"data\\/lm_vectors\\/colbert\\/corpus.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"nbits_2\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-07\\/05\\/04.03.18\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "[Jul 05, 04:03:34] #> Loading collection...\n",
      "0M \n",
      "[Jul 05, 04:03:46] [0] \t\t # of sampled PIDs = 59878 \t sampled_pids[:3] = [54607, 96034, 1332]\n",
      "[Jul 05, 04:03:46] [0] \t\t #> Encoding 59878 passages..\n",
      "[Jul 05, 04:03:59] [0] \t\t avg_doclen_est = 6.590333461761475 \t len(local_sample) = 59,878\n",
      "[Jul 05, 04:03:59] [0] \t\t Creating 8,192 partitions.\n",
      "[Jul 05, 04:03:59] [0] \t\t *Estimated* 769,144 embeddings.\n",
      "[Jul 05, 04:03:59] [0] \t\t #> Saving the indexing plan to colbert/indexes/nbits_2/plan.json ..\n",
      "Clustering 374886 points in 128D to 8192 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.04 s\n",
      "  Iteration 19 (1.86 s, search 1.59 s): objective=121214 imbalance=1.385 nsplit=0       \n",
      "[Jul 05, 04:04:02] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 05, 04:04:02] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.044, 0.041, 0.043, 0.042, 0.041, 0.042, 0.04, 0.041, 0.04, 0.043, 0.043, 0.042, 0.04, 0.044, 0.043, 0.043, 0.04, 0.04, 0.042, 0.041, 0.044, 0.042, 0.041, 0.042, 0.04, 0.041, 0.042, 0.041, 0.041, 0.042, 0.041, 0.042, 0.046, 0.041, 0.041, 0.039, 0.04, 0.041, 0.04, 0.04, 0.043, 0.043, 0.044, 0.044, 0.042, 0.043, 0.041, 0.046, 0.041, 0.041, 0.044, 0.041, 0.042, 0.045, 0.041, 0.044, 0.045, 0.042, 0.045, 0.043, 0.039, 0.041, 0.041, 0.046, 0.042, 0.042, 0.044, 0.045, 0.04, 0.041, 0.045, 0.043, 0.044, 0.041, 0.041, 0.044, 0.041, 0.042, 0.044, 0.043, 0.042, 0.041, 0.045, 0.043, 0.042, 0.04, 0.042, 0.042, 0.041, 0.043, 0.042, 0.043, 0.041, 0.045, 0.043, 0.043, 0.046, 0.041, 0.043, 0.043, 0.041, 0.041, 0.04, 0.042, 0.043, 0.041, 0.042, 0.042, 0.043, 0.04, 0.042, 0.04, 0.045, 0.039, 0.043, 0.041, 0.042, 0.043, 0.042, 0.041, 0.042, 0.041, 0.041, 0.042, 0.041, 0.047, 0.043, 0.041]\n",
      "[Jul 05, 04:04:02] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Jul 05, 04:04:02] #> Got bucket_cutoffs = tensor([-0.0327,  0.0002,  0.0331], device='cuda:0') and bucket_weights = tensor([-0.0591, -0.0148,  0.0152,  0.0596], device='cuda:0')\n",
      "[Jul 05, 04:04:02] avg_residual = 0.042144775390625\n",
      "0it [00:00, ?it/s][Jul 05, 04:04:02] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:04:07] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 164,957 embeddings. From #0 onward.\n",
      "1it [00:04,  4.62s/it][Jul 05, 04:04:07] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:04:12] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 164,423 embeddings. From #25,000 onward.\n",
      "2it [00:09,  4.71s/it][Jul 05, 04:04:12] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:04:16] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 165,282 embeddings. From #50,000 onward.\n",
      "3it [00:14,  4.73s/it][Jul 05, 04:04:16] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:04:22] [0] \t\t #> Saving chunk 3: \t 25,000 passages and 164,363 embeddings. From #75,000 onward.\n",
      "4it [00:19,  5.03s/it][Jul 05, 04:04:22] [0] \t\t #> Encoding 16708 passages..\n",
      "[Jul 05, 04:04:25] [0] \t\t #> Saving chunk 4: \t 16,708 passages and 110,001 embeddings. From #100,000 onward.\n",
      "5it [00:22,  4.56s/it]\n",
      "[Jul 05, 04:04:25] [0] \t\t #> Checking all files were saved...\n",
      "[Jul 05, 04:04:25] [0] \t\t Found all files!\n",
      "[Jul 05, 04:04:25] [0] \t\t #> Building IVF...\n",
      "[Jul 05, 04:04:25] [0] \t\t #> Loading codes...\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 2324.23it/s]\n",
      "[Jul 05, 04:04:25] [0] \t\t Sorting codes...\n",
      "[Jul 05, 04:04:25] [0] \t\t Getting unique codes...\n",
      "[Jul 05, 04:04:25] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 05, 04:04:25] #> Building the emb2pid mapping..\n",
      "[Jul 05, 04:04:25] len(emb2pid) = 769026\n",
      "100%|████████████████████████████████████| 8192/8192 [00:00<00:00, 82474.19it/s]\n",
      "[Jul 05, 04:04:26] #> Saved optimized IVF to colbert/indexes/nbits_2/ivf.pid.pt\n",
      "[Jul 05, 04:04:26] [0] \t\t #> Saving the indexing metadata to colbert/indexes/nbits_2/metadata.json ..\n",
      "#> Joined...\n",
      "[Jul 05, 04:04:37] #> Loading collection...\n",
      "0M \n",
      "[Jul 05, 04:04:48] #> Loading codec...\n",
      "[Jul 05, 04:04:48] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 05, 04:04:48] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 05, 04:04:49] #> Loading IVF...\n",
      "[Jul 05, 04:04:49] #> Loading doclens...\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 1269.46it/s]\n",
      "[Jul 05, 04:04:49] #> Loading codes and residuals...\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 313.12it/s]\n",
      "[Jul 05, 04:04:49] #> Loading the queries from data/lm_vectors/colbert/queries.tsv ...\n",
      "[Jul 05, 04:04:49] #> Got 116708 queries. All QIDs are unique.\n",
      "\n",
      "116708it [07:57, 244.29it/s]\n",
      "Saved nearest neighbors to data/lm_vectors/colbert/nearest_neighbor_kb_to_kb.p\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"4\" python src/colbertv2_knn.py --filename output/kb_to_kb.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query to kb similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"2,3\" python src/colbertv2_knn.py --filename output/query_to_kb.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Real Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: musique_facts_and_sim_graph_clean_facts_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: musique_similarity_edges_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_nodes_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: query_to_kb.tsv\n",
      "Removed: rel_kb_to_kb.tsv\n",
      "Removed: musique_facts_and_sim_graph_fact_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_passage_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: musique_facts_and_sim_graph_fact_doc_edges_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: kb_to_kb.tsv\n",
      "Removed: musique_facts_and_sim_graph_doc_to_facts_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_doc_to_facts_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_facts_to_phrases_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_facts_to_phrases_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_relation_dict_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.8_thresh_ents_only_lower_preprocess_ner_colbertv2.v3.subset.p\n"
     ]
    }
   ],
   "source": [
    "remove_new_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 11656/11656 [00:01<00:00, 10231.22it/s]\n",
      "Correct Wiki Format: 0 out of 11656\n",
      "Creating Graph\n",
      "100%|██████████████████████████████████| 11656/11656 [00:00<00:00, 38181.91it/s]\n",
      "Loading Vectors\n",
      "Augmenting Graph from Similarity\n",
      "100%|████████████████████████████████| 116708/116708 [00:01<00:00, 87846.65it/s]\n",
      "Saving Graph\n",
      "                                                         1\n",
      "0                                                         \n",
      "Total Phrases                                       327435\n",
      "Unique Phrases                                      113611\n",
      "Number of Individual Triples                        109145\n",
      "Number of Incorrectly Formatted Triples (ChatGP...    1136\n",
      "Number of Triples w/o NER Entities (ChatGPT Error)    6762\n",
      "Number of Unique Individual Triples                 107448\n",
      "Number of Entities                                  218290\n",
      "Number of Relations                                 239470\n",
      "Number of Unique Entities                            91729\n",
      "Number of Synonymy Edges                            135508\n",
      "Number of Unique Relations                           22222\n"
     ]
    }
   ],
   "source": [
    "!python src/create_graph_v2.py \\\n",
    "    --dataset musique \\\n",
    "    --model_name colbertv2 \\\n",
    "    --extraction_model gpt-3.5-turbo-1106 \\\n",
    "    --threshold 0.8 \\\n",
    "    --extraction_type ner \\\n",
    "    --create_graph \\\n",
    "    --cosine_sim_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jul 05, 04:16:20] #> Creating directory data/lm_vectors/colbert/musique/corpus/indexes/exp/colbertv2.0 \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \"data\\/lm_vectors\\/colbert\\/musique_corpus_11656.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"exp\\/colbertv2.0\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"data\\/lm_vectors\\/colbert\\/musique\",\n",
      "    \"experiment\": \"corpus\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-07\\/05\\/04.16.09\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "[Jul 05, 04:16:24] #> Loading collection...\n",
      "0M \n",
      "[Jul 05, 04:16:35] [0] \t\t # of sampled PIDs = 11656 \t sampled_pids[:3] = [6825, 166, 4892]\n",
      "[Jul 05, 04:16:35] [0] \t\t #> Encoding 11656 passages..\n",
      "[Jul 05, 04:16:48] [0] \t\t avg_doclen_est = 89.944580078125 \t len(local_sample) = 11,656\n",
      "[Jul 05, 04:16:48] [0] \t\t Creating 8,192 partitions.\n",
      "[Jul 05, 04:16:48] [0] \t\t *Estimated* 1,048,394 embeddings.\n",
      "[Jul 05, 04:16:48] [0] \t\t #> Saving the indexing plan to data/lm_vectors/colbert/musique/corpus/indexes/exp/colbertv2.0/plan.json ..\n",
      "Clustering 998394 points in 128D to 8192 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.10 s\n",
      "  Iteration 19 (4.02 s, search 3.46 s): objective=350642 imbalance=1.300 nsplit=0       \n",
      "[Jul 05, 04:16:53] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 05, 04:16:54] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.044, 0.045, 0.047, 0.043, 0.045, 0.044, 0.042, 0.04, 0.04, 0.044, 0.041, 0.044, 0.041, 0.043, 0.044, 0.043, 0.038, 0.04, 0.041, 0.043, 0.045, 0.043, 0.043, 0.043, 0.043, 0.042, 0.045, 0.043, 0.042, 0.046, 0.044, 0.045, 0.048, 0.043, 0.042, 0.039, 0.046, 0.044, 0.043, 0.051, 0.045, 0.043, 0.043, 0.042, 0.043, 0.043, 0.041, 0.047, 0.044, 0.041, 0.042, 0.042, 0.041, 0.044, 0.044, 0.045, 0.051, 0.043, 0.051, 0.041, 0.041, 0.045, 0.043, 0.046, 0.047, 0.043, 0.044, 0.045, 0.04, 0.042, 0.045, 0.042, 0.042, 0.044, 0.041, 0.043, 0.044, 0.048, 0.042, 0.044, 0.043, 0.043, 0.045, 0.045, 0.041, 0.043, 0.045, 0.045, 0.042, 0.049, 0.043, 0.047, 0.043, 0.043, 0.043, 0.043, 0.048, 0.041, 0.044, 0.044, 0.044, 0.046, 0.043, 0.044, 0.045, 0.045, 0.044, 0.042, 0.042, 0.041, 0.043, 0.045, 0.046, 0.041, 0.044, 0.042, 0.042, 0.043, 0.047, 0.045, 0.042, 0.042, 0.042, 0.046, 0.04, 0.042, 0.043, 0.041]\n",
      "[Jul 05, 04:16:54] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Jul 05, 04:16:54] #> Got bucket_cutoffs = tensor([-0.0339,  0.0008,  0.0357], device='cuda:0') and bucket_weights = tensor([-0.0606, -0.0154,  0.0170,  0.0626], device='cuda:0')\n",
      "[Jul 05, 04:16:54] avg_residual = 0.043548583984375\n",
      "0it [00:00, ?it/s][Jul 05, 04:16:54] [0] \t\t #> Encoding 11656 passages..\n",
      "[Jul 05, 04:17:06] [0] \t\t #> Saving chunk 0: \t 11,656 passages and 1,048,394 embeddings. From #0 onward.\n",
      "1it [00:12, 12.27s/it]\n",
      "[Jul 05, 04:17:06] [0] \t\t #> Checking all files were saved...\n",
      "[Jul 05, 04:17:06] [0] \t\t Found all files!\n",
      "[Jul 05, 04:17:06] [0] \t\t #> Building IVF...\n",
      "[Jul 05, 04:17:06] [0] \t\t #> Loading codes...\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 695.46it/s]\n",
      "[Jul 05, 04:17:06] [0] \t\t Sorting codes...\n",
      "[Jul 05, 04:17:06] [0] \t\t Getting unique codes...\n",
      "[Jul 05, 04:17:06] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 05, 04:17:06] #> Building the emb2pid mapping..\n",
      "[Jul 05, 04:17:06] len(emb2pid) = 1048394\n",
      "100%|████████████████████████████████████| 8192/8192 [00:00<00:00, 83018.60it/s]\n",
      "[Jul 05, 04:17:06] #> Saved optimized IVF to data/lm_vectors/colbert/musique/corpus/indexes/exp/colbertv2.0/ivf.pid.pt\n",
      "[Jul 05, 04:17:06] [0] \t\t #> Saving the indexing metadata to data/lm_vectors/colbert/musique/corpus/indexes/exp/colbertv2.0/metadata.json ..\n",
      "#> Joined...\n",
      "\n",
      "\n",
      "[Jul 05, 04:17:18] #> Creating directory data/lm_vectors/colbert/musique/phrase/indexes/exp/colbertv2.0 \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \"data\\/lm_vectors\\/colbert\\/musique_phrase_113611.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"exp\\/colbertv2.0\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"data\\/lm_vectors\\/colbert\\/musique\",\n",
      "    \"experiment\": \"phrase\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-07\\/05\\/04.16.09\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "[Jul 05, 04:17:22] #> Loading collection...\n",
      "0M \n",
      "[Jul 05, 04:17:33] [0] \t\t # of sampled PIDs = 59078 \t sampled_pids[:3] = [54607, 96034, 1332]\n",
      "[Jul 05, 04:17:33] [0] \t\t #> Encoding 59078 passages..\n",
      "[Jul 05, 04:17:46] [0] \t\t avg_doclen_est = 6.64111852645874 \t len(local_sample) = 59,078\n",
      "[Jul 05, 04:17:46] [0] \t\t Creating 8,192 partitions.\n",
      "[Jul 05, 04:17:46] [0] \t\t *Estimated* 754,504 embeddings.\n",
      "[Jul 05, 04:17:46] [0] \t\t #> Saving the indexing plan to data/lm_vectors/colbert/musique/phrase/indexes/exp/colbertv2.0/plan.json ..\n",
      "Clustering 372727 points in 128D to 8192 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.04 s\n",
      "  Iteration 19 (1.80 s, search 1.56 s): objective=120706 imbalance=1.384 nsplit=0       \n",
      "[Jul 05, 04:17:48] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jul 05, 04:17:49] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.044, 0.041, 0.043, 0.042, 0.042, 0.043, 0.04, 0.041, 0.04, 0.043, 0.043, 0.042, 0.041, 0.044, 0.043, 0.044, 0.04, 0.04, 0.042, 0.041, 0.044, 0.042, 0.041, 0.042, 0.04, 0.042, 0.042, 0.041, 0.04, 0.042, 0.041, 0.041, 0.045, 0.041, 0.041, 0.039, 0.04, 0.042, 0.041, 0.04, 0.042, 0.043, 0.044, 0.044, 0.043, 0.043, 0.041, 0.046, 0.041, 0.041, 0.043, 0.041, 0.042, 0.045, 0.041, 0.044, 0.046, 0.041, 0.044, 0.042, 0.039, 0.041, 0.041, 0.045, 0.042, 0.042, 0.044, 0.046, 0.04, 0.041, 0.045, 0.043, 0.043, 0.04, 0.041, 0.044, 0.041, 0.041, 0.043, 0.043, 0.042, 0.041, 0.045, 0.043, 0.042, 0.04, 0.042, 0.042, 0.041, 0.043, 0.042, 0.044, 0.042, 0.044, 0.044, 0.043, 0.046, 0.041, 0.044, 0.043, 0.041, 0.041, 0.04, 0.042, 0.043, 0.042, 0.042, 0.042, 0.043, 0.04, 0.041, 0.041, 0.045, 0.039, 0.043, 0.041, 0.042, 0.042, 0.043, 0.041, 0.043, 0.04, 0.041, 0.042, 0.041, 0.046, 0.043, 0.041]\n",
      "[Jul 05, 04:17:49] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Jul 05, 04:17:49] #> Got bucket_cutoffs = tensor([-0.0327,  0.0002,  0.0330], device='cuda:0') and bucket_weights = tensor([-0.0592, -0.0149,  0.0152,  0.0595], device='cuda:0')\n",
      "[Jul 05, 04:17:49] avg_residual = 0.0421142578125\n",
      "0it [00:00, ?it/s][Jul 05, 04:17:49] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:17:53] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 164,657 embeddings. From #0 onward.\n",
      "1it [00:04,  4.11s/it][Jul 05, 04:17:53] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:17:58] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 164,128 embeddings. From #25,000 onward.\n",
      "2it [00:08,  4.48s/it][Jul 05, 04:17:58] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:18:03] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 167,122 embeddings. From #50,000 onward.\n",
      "3it [00:13,  4.58s/it][Jul 05, 04:18:03] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jul 05, 04:18:07] [0] \t\t #> Saving chunk 3: \t 25,000 passages and 164,659 embeddings. From #75,000 onward.\n",
      "4it [00:18,  4.66s/it][Jul 05, 04:18:07] [0] \t\t #> Encoding 13611 passages..\n",
      "[Jul 05, 04:18:10] [0] \t\t #> Saving chunk 4: \t 13,611 passages and 93,603 embeddings. From #100,000 onward.\n",
      "5it [00:21,  4.24s/it]\n",
      "[Jul 05, 04:18:10] [0] \t\t #> Checking all files were saved...\n",
      "[Jul 05, 04:18:10] [0] \t\t Found all files!\n",
      "[Jul 05, 04:18:10] [0] \t\t #> Building IVF...\n",
      "[Jul 05, 04:18:10] [0] \t\t #> Loading codes...\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 2219.68it/s]\n",
      "[Jul 05, 04:18:10] [0] \t\t Sorting codes...\n",
      "[Jul 05, 04:18:10] [0] \t\t Getting unique codes...\n",
      "[Jul 05, 04:18:10] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jul 05, 04:18:10] #> Building the emb2pid mapping..\n",
      "[Jul 05, 04:18:11] len(emb2pid) = 754169\n",
      "100%|████████████████████████████████████| 8192/8192 [00:00<00:00, 83752.97it/s]\n",
      "[Jul 05, 04:18:11] #> Saved optimized IVF to data/lm_vectors/colbert/musique/phrase/indexes/exp/colbertv2.0/ivf.pid.pt\n",
      "[Jul 05, 04:18:11] [0] \t\t #> Saving the indexing metadata to data/lm_vectors/colbert/musique/phrase/indexes/exp/colbertv2.0/metadata.json ..\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"0\" python3 src/colbertv2_indexing.py \\\n",
    "    --dataset musique \\\n",
    "    --phrase output/musique_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p \\\n",
    "    --corpus data/musique_corpus.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: musique_facts_and_sim_graph_mean_0.6_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_clean_facts_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: musique_similarity_edges_mean_0.575_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_relation_dict_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_nodes_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: query_to_kb.tsv\n",
      "Removed: musique_facts_and_sim_graph_mean_0.475_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.575_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.55_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.525_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.5_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: rel_kb_to_kb.tsv\n",
      "Removed: musique_similarity_edges_mean_0.7_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_fact_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: musique_facts_and_sim_graph_mean_0.7_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_passage_chatgpt_openIE.ents_only_lower_preprocess_ner.v3.subset.json\n",
      "Removed: musique_facts_and_sim_graph_fact_doc_edges_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: kb_to_kb.tsv\n",
      "Removed: musique_similarity_edges_mean_0.6_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_similarity_edges_mean_0.5_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.4_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_doc_to_facts_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_similarity_edges_mean_0.8_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_doc_to_facts_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_facts_to_phrases_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_facts_to_phrases_csr_ents_only_lower_preprocess_ner.v3.subset.p\n",
      "Removed: musique_similarity_edges_mean_0.55_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_similarity_edges_mean_0.4_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_similarity_edges_mean_0.525_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_facts_and_sim_graph_mean_0.8_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n",
      "Removed: musique_similarity_edges_mean_0.475_thresh_ents_only_lower_preprocess_ner_grit-7b.v3.subset.p\n"
     ]
    }
   ],
   "source": [
    "remove_new_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 11656/11656 [00:01<00:00, 10070.51it/s]\n",
      "Correct Wiki Format: 0 out of 11656\n"
     ]
    }
   ],
   "source": [
    "!python src/create_graph_v2.py \\\n",
    "    --dataset musique \\\n",
    "    --model_name facebook/contriever \\\n",
    "    --extraction_model gpt-3.5-turbo-1106 \\\n",
    "    --threshold 0.8 \\\n",
    "    --extraction_type ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_to_kb.tsv\n",
      "rel_kb_to_kb.tsv\n",
      "kb_to_kb.tsv\n"
     ]
    }
   ],
   "source": [
    "show_new_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Pre-Computed Vectors. Confirming PLM Model.\n",
      "No Pre-Computed Vectors. Confirming PLM Model.\n",
      "No Pre-Computed Vectors. Confirming PLM Model.\n",
      "Loading PLM Vectors.\n",
      "Loading PLM Vectors.\n",
      "Loading PLM Vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.56it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.49it/s]\n",
      "100%|██████████| 44444/44444 [00:00<00:00, 2845631.79it/s]\n",
      "100%|██████████| 116432/116432 [00:00<00:00, 2794714.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 502 Missing Strings\n",
      "Encoding 1635 Missing Strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227222/227222 [00:00<00:00, 2716183.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 502 Missing Strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [00:00<00:00, 843.86it/s]\n",
      "100%|██████████| 502/502 [00:00<00:00, 852.60it/s]\n",
      "100%|██████████| 1635/1635 [00:00<00:00, 1891.95it/s]\n",
      "3it [00:01,  2.53it/s]\n",
      "  4%|▍         | 1682/44444 [00:00<00:02, 16814.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Vector Dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  2.06it/s]/44444 [00:00<00:01, 24656.96it/s]\n",
      "3it [00:01,  2.10it/s]\n",
      " 32%|███▏      | 14422/44444 [00:00<00:01, 24839.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating Vector Dict\n",
      "Populating Vector Dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44444/44444 [00:01<00:00, 25306.14it/s]]\n",
      " 29%|██▊       | 33257/116432 [00:01<00:03, 25578.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors Loaded.\n",
      "Chunking\n",
      "Building Index\n",
      "Running Index Part 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 148.17it/s]82it/s]\n",
      " 46%|████▋     | 54111/116432 [00:02<00:02, 26043.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 169.57it/s]18it/s]\n",
      " 35%|███▍      | 79395/227222 [00:03<00:05, 25799.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 168.30it/s]74it/s]\n",
      " 83%|████████▎ | 96182/116432 [00:03<00:00, 26155.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 171.09it/s].63it/s]\n",
      "100%|██████████| 116432/116432 [00:04<00:00, 25818.36it/s]\n",
      " 52%|█████▏    | 118209/227222 [00:04<00:04, 25773.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Index Chunks\n",
      "Vectors Loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 123358/227222 [00:04<00:04, 25684.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking\n",
      "Building Index\n",
      "Running Index Part 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 390.94it/s].47it/s]\n",
      " 60%|█████▉    | 136028/227222 [00:05<00:03, 24638.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 569.99it/s].27it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]0:03, 25520.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 535.07it/s].71it/s]\n",
      " 69%|██████▉   | 156573/227222 [00:06<00:02, 25414.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 582.44it/s].15it/s]\n",
      " 73%|███████▎  | 166885/227222 [00:06<00:02, 25672.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Index Chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2821it [00:00, 6506.64it/s]222 [00:07<00:01, 25683.84it/s]\n",
      "2821it [00:00, 4040.09it/s]222 [00:07<00:01, 25939.15it/s]\n",
      "100%|██████████| 227222/227222 [00:08<00:00, 25478.93it/s]\n",
      "17520it [00:02, 6266.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors Loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20649it [00:03, 6205.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking\n",
      "Building Index\n",
      "Running Index Part 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22222it [00:03, 6222.01it/s]0<00:05, 17.54it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 31.56it/s]\n",
      "13816it [00:03, 4213.09it/s]0<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22222it [00:05, 4275.78it/s]01<00:01, 33.53it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 33.38it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 34.54it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Index Part 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Index Chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113611it [00:17, 6660.62it/s]\n",
      "113611it [00:30, 3743.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processes completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义三个命令和对应的CUDA设备\n",
    "commands = [\n",
    "    (\"CUDA_VISIBLE_DEVICES=0 python src/RetrievalModule.py --retriever_name facebook/contriever --string_filename output/query_to_kb.tsv\", 0),\n",
    "    (\"CUDA_VISIBLE_DEVICES=1 python src/RetrievalModule.py --retriever_name facebook/contriever --string_filename output/rel_kb_to_kb.tsv\", 1),\n",
    "    (\"CUDA_VISIBLE_DEVICES=2 python src/RetrievalModule.py --retriever_name facebook/contriever --string_filename output/kb_to_kb.tsv\", 2),\n",
    "]\n",
    "\n",
    "# 启动每个命令\n",
    "processes = []\n",
    "for command, cuda_device in commands:\n",
    "    env = os.environ.copy()\n",
    "    env[\"CUDA_VISIBLE_DEVICES\"] = str(cuda_device)\n",
    "    process = subprocess.Popen(command, shell=True, env=env)\n",
    "    processes.append(process)\n",
    "\n",
    "# 等待所有进程完成\n",
    "for process in processes:\n",
    "    process.wait()\n",
    "\n",
    "print(\"All processes completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_recall_mean(output_path):\n",
    "    result = pd.read_json(output_path)\n",
    "    def recall_dict_to_columns(recall_dict):\n",
    "        return pd.Series({f'recall@{k}': v for k, v in recall_dict.items()})\n",
    "    recall_df = result['recall'].apply(recall_dict_to_columns)\n",
    "    return recall_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11656/11656 [00:01<00:00, 10427.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Wiki Format: 0 out of 11656\n",
      "Creating Graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11656/11656 [00:00<00:00, 37551.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vectors\n",
      "Augmenting Graph from Similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113611/113611 [00:03<00:00, 31457.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Graph\n",
      "                                                         1\n",
      "0                                                         \n",
      "Total Phrases                                       327435\n",
      "Unique Phrases                                      113611\n",
      "Number of Individual Triples                        109145\n",
      "Number of Incorrectly Formatted Triples (ChatGP...    1136\n",
      "Number of Triples w/o NER Entities (ChatGPT Error)    6762\n",
      "Number of Unique Individual Triples                 107448\n",
      "Number of Entities                                  218290\n",
      "Number of Relations                                 305529\n",
      "Number of Unique Entities                            91729\n",
      "Number of Synonymy Edges                            202124\n",
      "Number of Unique Relations                           22222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Graph: 100%|██████████| 585007/585007 [00:01<00:00, 579614.79it/s]\n",
      "2024-07-12 15:39:15,749 - hipporag_v2 - INFO - Graph built: num vertices: 113611, num_edges: 584896\n",
      "2024-07-12 15:39:15,794 - hipporag_v2 - INFO - Loading node vectors from: data/lm_vectors/facebook_contriever_mean/encoded_strings.txt\n",
      "2024-07-12 15:39:16,503 - hipporag_v2 - INFO - 1 phrases did not have vectors.\n",
      "IRCoT retrieval:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_no_ensemble_step_1_top_10_sim_thresh_0.8_damping_0.5.json'\n",
      "Results file maybe empty, cannot be loaded.\n",
      "Loaded 0 results from output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_no_ensemble_step_1_top_10_sim_thresh_0.8_damping_0.5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IRCoT retrieval: 100%|██████████| 1000/1000 [02:50<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 results to output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_no_ensemble_step_1_top_10_sim_thresh_0.8_damping_0.5.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python3', 'src/ircot_hipporag_v2.py', '--dataset', 'musique', '--retriever', 'facebook/contriever', '--max_steps', '1', '--doc_ensemble', 'f', '--top_k', '10', '--sim_threshold', '0.8', '--damping', '0.5'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export environment variables\n",
    "os.environ['OPENAI_API_KEY'] = 'aegsdvfhuijoak'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "create_graph_command = [\n",
    "    'python3', 'src/create_graph_v2.py',\n",
    "    '--dataset', 'musique',\n",
    "    '--model_name', 'facebook/contriever',\n",
    "    '--extraction_model', 'gpt-3.5-turbo-1106',\n",
    "    '--threshold', \"0.8\",\n",
    "    '--extraction_type', 'ner',\n",
    "    '--create_graph', '--cosine_sim_edges'\n",
    "]\n",
    "subprocess.run(create_graph_command, check=True)\n",
    "\n",
    "ircot_hipporag_command = [\n",
    "    'python3', 'src/ircot_hipporag_v2.py',\n",
    "    '--dataset', 'musique',\n",
    "    '--retriever', 'facebook/contriever',\n",
    "    '--max_steps', '1',\n",
    "    '--doc_ensemble', 'f',\n",
    "    '--top_k', '10',\n",
    "    '--sim_threshold', \"0.8\",\n",
    "    '--damping', '0.5',\n",
    "]\n",
    "subprocess.run(ircot_hipporag_command, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recall@1      0.275326\n",
       "recall@2      0.370912\n",
       "recall@5      0.465914\n",
       "recall@10     0.526998\n",
       "recall@15     0.526998\n",
       "recall@20     0.526998\n",
       "recall@30     0.526998\n",
       "recall@40     0.526998\n",
       "recall@50     0.526998\n",
       "recall@80     0.526998\n",
       "recall@100    0.526998\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_recall_mean('output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_no_ensemble_step_1_top_10_sim_thresh_0.8_damping_0.5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11656/11656 [00:01<00:00, 10467.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Wiki Format: 0 out of 11656\n",
      "Creating Graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11656/11656 [00:00<00:00, 36120.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vectors\n",
      "Augmenting Graph from Similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113611/113611 [00:03<00:00, 30887.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Graph\n",
      "                                                         1\n",
      "0                                                         \n",
      "Total Phrases                                       327435\n",
      "Unique Phrases                                      113611\n",
      "Number of Individual Triples                        109145\n",
      "Number of Incorrectly Formatted Triples (ChatGP...    1136\n",
      "Number of Triples w/o NER Entities (ChatGPT Error)    6762\n",
      "Number of Unique Individual Triples                 107448\n",
      "Number of Entities                                  218290\n",
      "Number of Relations                                 305529\n",
      "Number of Unique Entities                            91729\n",
      "Number of Synonymy Edges                            202124\n",
      "Number of Unique Relations                           22222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Graph: 100%|██████████| 585007/585007 [00:00<00:00, 590957.43it/s]\n",
      "2024-07-12 15:43:27,176 - hipporag_v2 - INFO - Graph built: num vertices: 113611, num_edges: 584896\n",
      "2024-07-12 15:43:27,220 - hipporag_v2 - INFO - Loading node vectors from: data/lm_vectors/facebook_contriever_mean/encoded_strings.txt\n",
      "2024-07-12 15:43:27,918 - hipporag_v2 - INFO - 1 phrases did not have vectors.\n",
      "2024-07-12 15:43:27,954 - hipporag_v2 - INFO - Loaded doc embeddings from data/lm_vectors/facebook_contriever_mean/musique_doc_embeddings.p, shape: (11656, 768)\n",
      "IRCoT retrieval:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_doc_ensemble_0.9_step_1_top_10_sim_thresh_0.8_damping_0.5.json'\n",
      "Results file maybe empty, cannot be loaded.\n",
      "Loaded 0 results from output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_doc_ensemble_0.9_step_1_top_10_sim_thresh_0.8_damping_0.5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IRCoT retrieval: 100%|██████████| 1000/1000 [02:57<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 results to output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_doc_ensemble_0.9_step_1_top_10_sim_thresh_0.8_damping_0.5.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python3', 'src/ircot_hipporag_v2.py', '--dataset', 'musique', '--retriever', 'facebook/contriever', '--max_steps', '1', '--doc_ensemble', 't', '--top_k', '10', '--sim_threshold', '0.8', '--damping', '0.5'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export environment variables\n",
    "os.environ['OPENAI_API_KEY'] = 'aegsdvfhuijoak'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "create_graph_command = [\n",
    "    'python3', 'src/create_graph_v2.py',\n",
    "    '--dataset', 'musique',\n",
    "    '--model_name', 'facebook/contriever',\n",
    "    '--extraction_model', 'gpt-3.5-turbo-1106',\n",
    "    '--threshold', \"0.8\",\n",
    "    '--extraction_type', 'ner',\n",
    "    '--create_graph', '--cosine_sim_edges'\n",
    "]\n",
    "subprocess.run(create_graph_command, check=True)\n",
    "\n",
    "ircot_hipporag_command = [\n",
    "    'python3', 'src/ircot_hipporag_v2.py',\n",
    "    '--dataset', 'musique',\n",
    "    '--retriever', 'facebook/contriever',\n",
    "    '--max_steps', '1',\n",
    "    '--doc_ensemble', 't',\n",
    "    '--top_k', '10',\n",
    "    '--sim_threshold', \"0.8\",\n",
    "    '--damping', '0.5',\n",
    "]\n",
    "subprocess.run(ircot_hipporag_command, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recall@1      0.296491\n",
       "recall@2      0.409497\n",
       "recall@5      0.526917\n",
       "recall@10     0.622585\n",
       "recall@15     0.622585\n",
       "recall@20     0.622585\n",
       "recall@30     0.622585\n",
       "recall@40     0.622585\n",
       "recall@50     0.622585\n",
       "recall@80     0.622585\n",
       "recall@100    0.622585\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_recall_mean('output/ircot_v2/ircot_results_musique_hipporag_facebook_contriever_demo_1_gpt-3_5-turbo-1106_doc_ensemble_0.9_step_1_top_10_sim_thresh_0.8_damping_0.5.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
